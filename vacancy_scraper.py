import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
import json
import os
import re
from urllib.parse import quote_plus

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CONFIG = {
    "telegram_bot_token": "7739167340:AAHfiWC6F29K7EDGP4ugnGv0d9S0e-j3Qlk",
    "telegram_chat_id": "340159288",
    "search_queries": ["python junior —É–¥–∞–ª–µ–Ω–Ω–æ", "python —Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∞ —É–¥–∞–ª–µ–Ω–Ω–æ"],
    "check_interval": 3600,  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—ã–µ 3600 —Å–µ–∫—É–Ω–¥ (1 —á–∞—Å)
    "data_file": "vacancies.json",
    "custom_sites": [
        {
            "name": "HH.ru",
            "url_template": "https://hh.ru/search/vacancy?text={query}&schedule=remote",
            "parser": "parse_hh"
        },
        {
            "name": "Habr Career",
            "url_template": "https://career.habr.com/vacancies?q={query}&remote=true",
            "parser": "parse_habr"
        },
        {
            "name": "Rabota.ru",
            "url_template": "https://rabota.ru/vacancy/?query={query}&employment=remote",
            "parser": "parse_rabota"
        },
        {
            "name": "Remote job",
            "url_template": "https://remote-job.ru/search?search%5Bquery%5D={query}&search%5BsearchType%5D=vacancy",
            "parser": "parse_remote_job"
        },
        # –î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ —Å–∞–π—Ç—ã –ø–æ –æ–±—Ä–∞–∑—Ü—É:
        # {
        #     "name": "–ù–∞–∑–≤–∞–Ω–∏–µ —Å–∞–π—Ç–∞",
        #     "url_template": "URL —Å {query} –≤–º–µ—Å—Ç–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞",
        #     "parser": "–≤–∞—à–∞_—Ñ—É–Ω–∫—Ü–∏—è_–ø–∞—Ä—Å–µ—Ä–∞"
        # }
    ]
}


class VacancyScraper:
    def __init__(self):
        self.sent_vacancies = self.load_sent_vacancies()
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        })

    def load_sent_vacancies(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        if os.path.exists(CONFIG["data_file"]):
            with open(CONFIG["data_file"], "r", encoding="utf-8") as f:
                return set(json.load(f))
        return set()

    def save_sent_vacancies(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ —Ñ–∞–π–ª"""
        with open(CONFIG["data_file"], "w", encoding="utf-8") as f:
            json.dump(list(self.sent_vacancies), f)

    def clean_text(self, text):
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫"""
        if not text:
            return ""
        return re.sub(r'\s+', ' ', text).strip()

    def parse_remote_job(self, html):
        """–ü–∞—Ä—Å–µ—Ä –¥–ª—è remote-job.ru"""
        try:
            # –∫–æ–¥ –ø–∞—Ä—Å–µ—Ä–∞
            soup = BeautifulSoup(html, "html.parser")
            vacancies = []

            for item in soup.find_all("div", class_="vacancy-card"):
                title_tag = item.find("h3", class_="vacancy-card__title")
                if not title_tag:
                    continue

                title = self.clean_text(title_tag.text)
                link = "https://remote-job.ru" + title_tag.find("a")["href"]
                company = item.find("div", class_="vacancy-card__company-name")
                company = self.clean_text(company.text) if company else "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
                location = item.find("div", class_="vacancy-card__locations")
                location = self.clean_text(location.text) if location else "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
                salary = item.find("div", class_="vacancy-card__salary")
                salary = self.clean_text(salary.text) if salary else "–ù–µ —É–∫–∞–∑–∞–Ω–∞"

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–∞–∫–∞–Ω—Å–∏—è —É–¥–∞–ª—ë–Ω–Ω–∞—è
                if "—É–¥–∞–ª–µ–Ω" not in location.lower() and "remote" not in location.lower():
                    continue

                vacancy_id = link.split("/")[-1]

                vacancies.append({
                    "id": f"remotejob_{vacancy_id}",
                    "title": title,
                    "company": company,
                    "location": location,
                    "salary": salary,
                    "link": link,
                    "requirements": "–°–º. –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Å–∞–π—Ç–µ",
                    "source": "remote-job.ru"
                })
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {site_config['name']}: {e}")
            return []


        return vacancies

    def parse_rabota(self, html):
        """–ü–∞—Ä—Å–µ—Ä –¥–ª—è rabota.ru"""
        soup = BeautifulSoup(html, "html.parser")
        vacancies = []

        for item in soup.find_all("article", class_="vacancy-preview-card"):
            title_tag = item.find("h3", class_="vacancy-preview-card__title")
            if not title_tag:
                continue

            title = self.clean_text(title_tag.text)
            link = "https://rabota.ru" + title_tag.find("a")["href"]
            company = item.find("span", class_="vacancy-preview-card__company-name")
            company = self.clean_text(company.text) if company else "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
            location = item.find("span", class_="vacancy-preview-card__location")
            location = self.clean_text(location.text) if location else "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
            salary = item.find("span", class_="vacancy-preview-card__salary")
            salary = self.clean_text(salary.text) if salary else "–ù–µ —É–∫–∞–∑–∞–Ω–∞"

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤–∞–∫–∞–Ω—Å–∏—è —É–¥–∞–ª—ë–Ω–Ω–∞—è
            if "—É–¥–∞–ª–µ–Ω" not in location.lower() and "remote" not in location.lower():
                continue

            vacancy_id = link.split("/")[-2]

            vacancies.append({
                "id": f"rabota_{vacancy_id}",
                "title": title,
                "company": company,
                "location": location,
                "salary": salary,
                "link": link,
                "requirements": "–°–º. –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Å–∞–π—Ç–µ",
                "source": "Rabota.ru"
            })

        return vacancies

    def parse_hh(self, html):
        """–ü–∞—Ä—Å–µ—Ä –¥–ª—è hh.ru"""
        soup = BeautifulSoup(html, "html.parser")
        vacancies = []

        for item in soup.find_all("div", class_="vacancy-serp-item-body"):
            title_tag = item.find("a", class_="serp-item__title")
            if not title_tag:
                continue

            title = self.clean_text(title_tag.text)
            link = title_tag["href"].split("?")[0]
            company = item.find("a", class_="bloko-link_kind-tertiary")
            company = self.clean_text(company.text) if company else "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
            location = item.find("div", {"data-qa": "vacancy-serp__vacancy-address"})
            location = self.clean_text(location.text) if location else "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
            salary = item.find("span", {"data-qa": "vacancy-serp__vacancy-compensation"})
            salary = self.clean_text(salary.text) if salary else "–ù–µ —É–∫–∞–∑–∞–Ω–∞"

            if "—É–¥–∞–ª–µ–Ω" not in location.lower() and "remote" not in location.lower():
                continue

            vacancy_id = link.split("/")[-1]
            requirements = self.get_hh_requirements(link) if "hh.ru" in link else "–ù–µ —É–∫–∞–∑–∞–Ω—ã"

            vacancies.append({
                "id": vacancy_id,
                "title": title,
                "company": company,
                "location": location,
                "salary": salary,
                "link": link,
                "requirements": requirements,
                "source": "hh.ru"
            })

        return vacancies

    def get_hh_requirements(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Å hh.ru"""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            section = soup.find("div", {"data-qa": "vacancy-description"})
            if not section:
                return "–ù–µ —É–∫–∞–∑–∞–Ω—ã"

            text = self.clean_text(section.text)
            return text[:1000] + "..." if len(text) > 1000 else text
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π —Å hh.ru: {e}")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è"

    def parse_habr(self, html):
        """–ü–∞—Ä—Å–µ—Ä –¥–ª—è career.habr.com"""
        soup = BeautifulSoup(html, "html.parser")
        vacancies = []

        for item in soup.find_all("div", class_="vacancy-card"):
            title_tag = item.find("a", class_="vacancy-card__title-link")
            if not title_tag:
                continue

            title = self.clean_text(title_tag.text)
            link = "https://career.habr.com" + title_tag["href"]
            company = item.find("a", class_="vacancy-card__company-title")
            company = self.clean_text(company.text) if company else "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
            meta = item.find("div", class_="vacancy-card__meta")
            location = self.clean_text(meta.text) if meta else "–ù–µ —É–∫–∞–∑–∞–Ω–æ"
            salary = item.find("div", class_="vacancy-card__salary")
            salary = self.clean_text(salary.text) if salary else "–ù–µ —É–∫–∞–∑–∞–Ω–∞"

            if "—É–¥–∞–ª–µ–Ω" not in location.lower() and "remote" not in location.lower():
                continue

            vacancy_id = link.split("/")[-1]
            requirements = self.get_habr_requirements(link)

            vacancies.append({
                "id": vacancy_id,
                "title": title,
                "company": company,
                "location": location,
                "salary": salary,
                "link": link,
                "requirements": requirements,
                "source": "career.habr.com"
            })

        return vacancies

    def get_habr_requirements(self, url):
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Å career.habr.com"""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            section = soup.find("div", class_="vacancy-description__text")
            if not section:
                return "–ù–µ —É–∫–∞–∑–∞–Ω—ã"

            text = self.clean_text(section.text)
            return text[:1000] + "..." if len(text) > 1000 else text
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π —Å career.habr.com: {e}")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è"

    def scrape_site(self, site_config, query):
        """–û–±—â–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —Å–∞–π—Ç–∞"""
        try:
            url = site_config["url_template"].format(query=quote_plus(query))
            response = self.session.get(url)
            response.raise_for_status()

            parser = getattr(self, site_config["parser"])
            return parser(response.text)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {site_config['name']}: {e}")
            return []

    def send_to_telegram(self, vacancy):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤–∞–∫–∞–Ω—Å–∏—é –≤ Telegram"""
        message = (
            f"üè¢ <b>{vacancy['title']}</b>\n"
            f"üìå <b>–ö–æ–º–ø–∞–Ω–∏—è:</b> {vacancy['company']}\n"
            f"üí∞ <b>–ó–∞—Ä–ø–ª–∞—Ç–∞:</b> {vacancy['salary']}\n"
            f"üåç <b>–õ–æ–∫–∞—Ü–∏—è:</b> {vacancy['location']}\n"
            f"üîó <a href='{vacancy['link']}'>–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é</a>\n"
            f"üìå <b>–ò—Å—Ç–æ—á–Ω–∏–∫:</b> {vacancy['source']}\n\n"
            f"üìù <b>–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:</b>\n{vacancy['requirements']}"
        )

        url = f"https://api.telegram.org/bot{CONFIG['telegram_bot_token']}/sendMessage"
        params = {
            "chat_id": CONFIG["telegram_chat_id"],
            "text": message,
            "parse_mode": "HTML",
            "disable_web_page_preview": True
        }

        try:
            response = requests.post(url, params=params)
            response.raise_for_status()
            print(f"–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Telegram: {vacancy['title']}")
            return True
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –≤ Telegram: {e}")
            return False

    def process_vacancies(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–π"""
        all_vacancies = []

        for query in CONFIG["search_queries"]:
            for site in CONFIG["custom_sites"]:
                all_vacancies.extend(self.scrape_site(site, query))

        new_vacancies = 0

        for vacancy in all_vacancies:
            if vacancy["id"] not in self.sent_vacancies:
                if self.send_to_telegram(vacancy):
                    self.sent_vacancies.add(vacancy["id"])
                    new_vacancies += 1

        if new_vacancies > 0:
            self.save_sent_vacancies()

        print(f"–ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π: {new_vacancies}")

    def run(self):
        """–ó–∞–ø—É—Å–∫ —Å–∫—Ä–∞–ø–µ—Ä–∞ –≤ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–º —Ü–∏–∫–ª–µ"""
        print("–°–∫—Ä–∞–ø–µ—Ä –≤–∞–∫–∞–Ω—Å–∏–π –∑–∞–ø—É—â–µ–Ω. –î–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C")
        print(f"–ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∞–π—Ç—ã: {[s['name'] for s in CONFIG['custom_sites']]}")

        try:
            while True:
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - –ù–∞—á–∏–Ω–∞—é –ø—Ä–æ–≤–µ—Ä–∫—É –≤–∞–∫–∞–Ω—Å–∏–π...")
                self.process_vacancies()
                print(f"–°–ª–µ–¥—É—é—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ {CONFIG['check_interval']} —Å–µ–∫—É–Ω–¥...")
                time.sleep(CONFIG["check_interval"])
        except KeyboardInterrupt:
            print("–°–∫—Ä–∞–ø–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")


if __name__ == "__main__":
    # –ü–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –∑–∞–º–µ–Ω–∏—Ç–µ YOUR_TELEGRAM_BOT_TOKEN –∏ YOUR_TELEGRAM_CHAT_ID

    # –ü—Ä–∏–º–µ—Ä –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Å–∞–π—Ç–∞:
    # CONFIG["custom_sites"].append({
    #     "name": "–ù–æ–≤—ã–π —Å–∞–π—Ç",
    #     "url_template": "https://example.com/search?q={query}&remote=1",
    #     "parser": "parse_new_site"  # –ù—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –º–µ—Ç–æ–¥ parse_new_site(self, html)
    # })

    scraper = VacancyScraper()
    scraper.run()
